{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1829aabc",
   "metadata": {},
   "source": [
    "### 상관계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0499b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#판다스로 상관관계 분석\n",
    "import pandas as pd\n",
    "\n",
    "df[var_list].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0aca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairplot 그리기\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df[var_list], kind=\"reg\", height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy로 상관계수 계산 & 검정\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = stats.pearsonr(df[\"y\"], df[\"x\"])\n",
    "sns.regplot(x=df[\"x\"], y=df[\"y\"])\n",
    "plt.show()\n",
    "print(\"상관계수 : {}, p-value : {}\".format(r[0], r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#상관계수 행렬을 heatmap으로 표현하기\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "colormap = plt.cm.PuBu\n",
    "sns.heatmap(df[var_list].corr(), cmap=colormap, annot = True, annot_kws = {\"size\" : 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330182cf",
   "metadata": {},
   "source": [
    "### 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ols(최소자승법) 다중회귀 분석\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(formula=\"종속변수 ~ 독립변수1 + 독립변수2\", data=df)\n",
    "result = model.fit()\n",
    "result.summary()\n",
    "result.aic #AIC 확인\n",
    "result.rsquared_adj #수정된 r-square 확인\n",
    "result.pvalues #p-value 확인 (배열로 접근하면 변수별로 확인 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다중공선성 확인을 위해 VIF를 구하겠다면 아래와 같이\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "variance_inflation_factor(model.exog, i)\n",
    "\n",
    "#VIF가 10이 넘으면 다중공선성 있다고 판단하며 5가 넘으면 주의할 필요가 있는 것으로 봅니다.\n",
    "#독립 변수 a와 b가 서로 상관 관계가 있다고 했을 때 두 변수 모두 VIF가 높습니다.\n",
    "#어느 하나만 VIF가 높은 경우는 없습니다.\n",
    "#박수도 오른손과 왼손이 있어야 칠 수 있듯이 서로 연관 있는 변수끼리 VIF가 높습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#후진제거법\n",
    "import statsmodels.formula.api as smf\n",
    "def backward_elimination(df, dep_var, ind_vars, select_criteria = \"AIC\", return_case = \"selected_vars\"):\n",
    "    alpha = 0.05\n",
    "    selected_vars = ind_vars.copy()\n",
    "    eliminated_vars = []\n",
    "    done = False\n",
    "    \n",
    "    while done is False:\n",
    "        #step1 : 변수 하나씩을 제외한 n개의 모델을 만들어낸다. (n : 변수 갯수)\n",
    "        model_result_list = []\n",
    "        for var in ind_vars:\n",
    "            var_list = ind_vars.copy()\n",
    "            if len(eliminated_vars)>0:\n",
    "                var_list.remove(eliminated_vars)\n",
    "            var_list.remove(var)\n",
    "            this_ind_vars_str = \" + \".join(var_list)\n",
    "            this_model_fit = smf.ols(dep_var+\" ~ \"+this_ind_vars_str, data=df).fit()\n",
    "            this_result = {\n",
    "                \"selected var\" : this_ind_vars_str,\n",
    "                \"eliminated var\" : var,            \n",
    "                \"AIC\" : this_model_fit.aic,\n",
    "                \"rsquared_adj\" : this_model_fit.rsquared_adj\n",
    "            }\n",
    "            model_result_list.append(this_result)\n",
    "        result = pd.DataFrame(model_result_list)\n",
    "        #step2 : n개 모델들 중 가장 좋은 결과를 보인 모델을 선택하고, 제외 후보 변수를 찾는다.\n",
    "        ascending = True\n",
    "        if select_criteria == \"rsquared_adj\":\n",
    "            ascending = False\n",
    "        result.sort_values(by=select_criteria, ascending=ascending, inplace=True)\n",
    "        candidate_var = result.iloc[0, [1]].values[0]\n",
    "        #step3 : 제외 후보 변수로 fitting하여 p-value를 알아보고 유의하다면 종료한다.\n",
    "        candidate_model_fit = smf.ols(dep_var+\" ~ \"+candidate_var, data=df).fit()\n",
    "        if candidate_model_fit.pvalues[candidate_var] > alpha: #유의하지 않다면\n",
    "            eliminated_vars.append(candidate_var) #제거확정\n",
    "            selected_vars.remove(candidate_var)\n",
    "        else: #유의하다면 종료한다\n",
    "            done = True\n",
    "    if return_case == \"s\":\n",
    "        print(\"후보 독립변수들 : {}\".format(ind_vars))\n",
    "        print(\"후진제거법 선택변수 : {}\".format(selected_vars))\n",
    "        return selected_vars\n",
    "    elif return_case == \"e\":\n",
    "        return eliminated_vars\n",
    "\n",
    "backward_elimination(df, dep_var, ind_vars, \"AIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전진 선택법 (단계적 선택법도 옵션에 포함)\n",
    "import statsmodels.formula.api as smf\n",
    "def forward_selection(df, dep_var, ind_vars, select_criteria = \"AIC\", stepwise=False):\n",
    "    if stepwise:\n",
    "        #후진제거법으로 제거되는 변수를 먼저 찾아 놓는다.\n",
    "        eliminated_vars = backward_elimination(df, dep_var, ind_vars, \"AIC\", \"e\")\n",
    "    candidate_vars = ind_vars.copy()\n",
    "    selected_vars = [] #전진선택법으로 선택된 변수들\n",
    "    alpha = 0.05 #새롭게 편입된 변수의 회귀계수 p-value가 0.05보다 작을 경우에만 선택\n",
    "    \n",
    "    while len(candidate_vars) > 0: #후보 변수 리스트가 모두 소거될 때까지 반복한다.\n",
    "        candidate_vars_reg_result = []\n",
    "        for candidate in candidate_vars:\n",
    "            #새로운 변수가 추가되었을 때 회귀모형의 AIC와 r-square값을 구해본다.\n",
    "            this_ind_vars_str = candidate\n",
    "            if len(selected_vars) > 0:\n",
    "                this_ind_vars_str = \" + \".join(selected_vars) + \" + \" + candidate\n",
    "            this_model_fit = smf.ols(dep_var+\" ~ \"+this_ind_vars_str, data=df).fit()\n",
    "            this_result = {\n",
    "                \"var\" : candidate,\n",
    "                \"AIC\" : this_model_fit.aic,\n",
    "                \"rsquared_adj\" : this_model_fit.rsquared_adj,\n",
    "                \"p-value\" : this_model_fit.pvalues[candidate]\n",
    "            }\n",
    "            candidate_vars_reg_result.append(this_result)\n",
    "        result = pd.DataFrame(candidate_vars_reg_result)\n",
    "        ascending = True\n",
    "        if select_criteria == \"rsquared_adj\":\n",
    "            ascending = False\n",
    "        result.sort_values(by=select_criteria, ascending=ascending, inplace=True)\n",
    "        selected_var = result.iloc[0,[0]].values[0]\n",
    "        selected_var_pvalue = result.iloc[0,[3]].values[0]\n",
    "        if selected_var_pvalue > alpha:\n",
    "            break\n",
    "        selected_vars.append(selected_var)\n",
    "        candidate_vars.remove(selected_var)\n",
    "        if stepwise:\n",
    "            #selected_vars 중에 eliminated_var에 해당하는 것이 있다면 제거한다\n",
    "            if len(selected_vars) > 2 and len(eliminated_vars) > 0:\n",
    "                for ev in eliminated_vars:\n",
    "                    if ev in selected_vars:\n",
    "                        selected_vars.remove(ev)\n",
    "    print(\"후보 독립변수들 : {}\".format(ind_vars))\n",
    "    if stepwise:\n",
    "        print(\"단계적 선택법 선택변수 : {}\".format(selected_vars))\n",
    "    else:\n",
    "        print(\"전진선택법 선택변수 : {}\".format(selected_vars))\n",
    "    return selected_vars\n",
    "\n",
    "forward_selection(df, dep_var, ind_vars, \"AIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78131394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단계적 선택법\n",
    "forward_selection(df, dep_var, ind_vars, \"AIC\", stepwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b5174",
   "metadata": {},
   "source": [
    "### 회귀분석 - 잔차의 독립성, 정규성, 등분산성 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae894b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#잔차의 독립성 검증\n",
    "\n",
    "#잔차의 독립성 : 잔차들끼리 상관관계가 없다는 것을 의미. 즉, 자기상관이 없다는 것\n",
    "#자기상관이 있다면 F값과 t값, r스퀘어 값이 과장되어 나타날 수 있음\n",
    "#잔차의 독립성이 확보되어야 회귀모형의 검증력이 인정을 받음\n",
    "\n",
    "#방법 : Durbin-watson 검정으로 체크\n",
    "#더빈왓슨 수치의 해석 : 0 ~ 4 사이의 값을 가지며 2와 가까울수록 독립성이 있다고 봄.\n",
    "#0이나 4에 가까운 숫자라면 독립성이 없다고 봄\n",
    "#summary()에 있는 더빈왓슨 결과값을 확인하면 됨\n",
    "\n",
    "model_fit = smf.ols(\"종속변수 ~ 독립변수\", data=df).fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d98ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잔차의 정규성 검증\n",
    "\n",
    "# 1.샤피로 윌크 검정으로 체크. p-value가 0.05보다 작다면 귀무가설(=정규성이 있다)을 기각\n",
    "#   예를 들어 p-value가 1.44e-13으로 0.05보다 낮게 나올 경우 정규성이 없다고 판단\n",
    "from scipy import stats\n",
    "\n",
    "model_fit = smf.ols(\"종속변수 ~ 독립변수\", data=df).fit()\n",
    "\n",
    "predict = model_fit.predict(df)\n",
    "predict.name=\"predict\"\n",
    "\n",
    "residual = df[dep_var] - predict\n",
    "residual.name=\"residual\"\n",
    "\n",
    "stats.shapiro(residual)\n",
    "\n",
    "\n",
    "# 2. Q-Q plot으로 시각화하여 정규성 체크\n",
    "#    X축은 정규분포곡선의 확률분포 위치값\n",
    "#    Y축은 잔차가 정규분포곡선의 분포와 비교해 얼마나 떨어져 있는지의 값\n",
    "#    붉은선에서 벗어나 있으면(대각선이 아니면) 정규성 없다고 봄\n",
    "\n",
    "stats.probplot(residual, plot=plt)\n",
    "plt.title(\"Q-Q plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b65e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잔차의 등분산성 검증\n",
    "\n",
    "#붉은색 선이 직선에 가까우면 등분산으로 볼 수 있음\n",
    "sns.regplot(x=predict, y=residual, lowess=True, line_kws={'color': 'red'})\n",
    "plt.plot([predict.min(), predict.max()], [0, 0], '--', color='grey')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
